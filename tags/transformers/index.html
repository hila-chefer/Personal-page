<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academia 4.3.1"><meta name=generator content="Hugo 0.104.3"><meta name=author content="Hila Chefer"><meta name=description content><link rel=alternate hreflang=en-us href=https://examplesite.org/tags/transformers/><meta name=theme-color content="#b54845"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap"><link rel=stylesheet href=/css/academia.min.d1d8d530dfa7b55780d5887faa05b19b.css><link rel=alternate href=/tags/transformers/index.xml type=application/rss+xml title="Hila Chefer"><link rel=feed href=/tags/transformers/index.xml type=application/rss+xml title="Hila Chefer"><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://examplesite.org/tags/transformers/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@hila_chefer"><meta property="twitter:creator" content="@hila_chefer"><meta property="og:site_name" content="Hila Chefer"><meta property="og:url" content="https://examplesite.org/tags/transformers/"><meta property="og:title" content="Transformers | Hila Chefer"><meta property="og:description" content><meta property="og:image" content="https://examplesite.org/img/icon-192.png"><meta property="twitter:image" content="https://examplesite.org/img/icon-192.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-11-20T00:00:00+00:00"><title>Transformers | Hila Chefer</title></head><body id=top data-spy=scroll data-target=#TableOfContents data-offset=71><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main><div class=container><a class=navbar-brand href=/>Hila Chefer</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class="universal-wrapper py-3"><h1 itemprop=name>Transformers</h1></div></div></div><div class=universal-wrapper><div><h2><a href=/publication/optimizing_relevance_for_robustness/>Optimizing Relevance Maps of Vision Transformers Improves Robustness</a></h2><div class=article-style>Vision models are known to use "shortcuts" in the data, i.e. use irrelevant cues, such as the image background, to achieve high accuracy. In this work, we show that using a very short and simple *few-shot* finetuning process on the relevance maps of a Vision Transformer, we can teach the model *why* the label is correct, and enforce that the predictions are based on the *right* reasons. We demonstrate a significant improvement in the robustness of the Vision Transformers (ViTs) to distribution shifts.</div></div><div><h2><a href=/publication/clip-guided-essence-transfer/>Image-Based Clip-Guided Essence Transfer</a></h2><div class=article-style>This paper proposes a novel method to transfer the semantic properties that constitute high-level textual description from a target image to a source image, without changing the identity of the source. The method uses CLIP's image latent space, which is more stable and expressive than the textual latent space.</div></div><div><h2><a href=/publication/no-token-left-behind/>No Token Left Behind: Explainability-Aided Image Classification and Generation</a></h2><div class=article-style>The paper presents a novel use of explainability to perform zero-shot tasks such as image classification and generation. We demonstrate that CLIP guidance based on pure similarity scores between the image and text is unstable as the scores can be based on irrelevant or partial data. Our method demonstrates the effectiveness of using explainability to stabilize the scores.</div></div><div><h2><a href=/talk/columbia-08-2022/>Transformer Explainability Beyond Accountability (English)</a></h2><div class=article-style>This talk takes a deep dive into Transformer explainability algorithms, and demonstrates how explainability can be used to improve downstream tasks such as image editing, and even increase robustness and accuracy of image backbones.</div></div><div><h2><a href=/talk/microsoft-01-2022/>Intro to Transformers and Transformer Explainability (English)</a></h2><div class=article-style>This talk takes a deep dive into the attention mechanism. During the talk, we review the motivations and applications of the self-attention mechanism. Additionally, we review the main building blocks for self-attention explainability and some cool applications of Transformer-explainability from recent research.</div></div><div><h2><a href=/talk/imvc-10-2021/>Transformer Explainability (English)</a></h2><div class=article-style>This talk explores the main milestones in Transformer-based research, and Transformer explainability research.</div></div><div><h2><a href=/talk/ibm-10-2021/>Transformer Explainability (Hebrew)</a></h2><div class=article-style>This talk explores the main milestones in Transformer-based research, and Transformer explainability research.</div></div><div><h2><a href=/publication/generic-attention-model-explainability/>Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers (Oral)</a></h2><div class=article-style>The paper presents an interpretability method for all types of attention, including bi-modal Transformers and encoder-decoder Transformers. The method achieves SOTA results for CLIP, DETR, LXMERT, and more.</div></div><div><h2><a href=/publication/transformer-interpretability/>Transformer Interpretability Beyond Attention Visualization</a></h2><div class=article-style>This paper presents an interpretability method for self-attention based models, and specifically for Transformer encoders. The method incorporates LRP and gradients, and achieves SOTA results for ViT, BERT, and DeiT.</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
<script>hljs.initHighlightingOnLoad()</script><script src=/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js></script><div class=container><footer class=site-footer><div class=container><div class="row align-items-center"><div class="col-md-6 mb-4 mb-md-0"><p class=mb-0>Copyright Â© 2022 &#183;
Powered by
<a href=https://gethugothemes.com target=_blank rel=noopener>Gethugothemes</a></p></div><div class=col-md-6><ul class="list-inline network-icon text-right mb-0"><li class=list-inline-item><a href=https://github.com/hila-chefer target=_blank rel=noopener title=Github><i class="fab fa-github" aria-hidden=true></i></a></li><li class=list-inline-item><a href="https://scholar.google.com/citations?user=B8sA9JoAAAAJ&hl=en" target=_blank rel=noopener title="Google Scholar"><i class="ai ai-google-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.semanticscholar.org/author/Hila-Chefer/2038268012 target=_blank rel=noopener title="Semantic Scholar"><i class="ai ai-semantic-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://twitter.com/hila_chefer target=_blank rel=noopener title=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.linkedin.com/in/hila-chefer target=_blank rel=noopener title=LinkedIn><i class="fab fa-linkedin" aria-hidden=true></i></a></li></ul></div></div></div></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>