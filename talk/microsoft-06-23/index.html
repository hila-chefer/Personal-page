<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Source Themes Academia 4.3.1"><meta name=generator content="Hugo 0.121.2"><meta name=author content="Hila Chefer"><meta name=description content="This talk takes a deep dive into Attend-and-Excite. The paper presents a method to guide text-to-image diffusion models to generate all subjects in the input prompt, to mitigate subject neglect. This is achieved by defining an intuitive loss over the cross-attention maps during inference without any additional data or fine-tuning."><link rel=alternate hreflang=en-us href=https://examplesite.org/talk/microsoft-06-23/><meta name=theme-color content="#b54845"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap"><link rel=stylesheet href=/css/academia.min.d1d8d530dfa7b55780d5887faa05b19b.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://examplesite.org/talk/microsoft-06-23/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@hila_chefer"><meta property="twitter:creator" content="@hila_chefer"><meta property="og:site_name" content="Hila Chefer"><meta property="og:url" content="https://examplesite.org/talk/microsoft-06-23/"><meta property="og:title" content="Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-image Diffusion Models (English) | Hila Chefer"><meta property="og:description" content="This talk takes a deep dive into Attend-and-Excite. The paper presents a method to guide text-to-image diffusion models to generate all subjects in the input prompt, to mitigate subject neglect. This is achieved by defining an intuitive loss over the cross-attention maps during inference without any additional data or fine-tuning."><meta property="og:image" content="https://examplesite.org/img/icon-192.png"><meta property="twitter:image" content="https://examplesite.org/img/icon-192.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-07-02T13:00:00+00:00"><meta property="article:modified_time" content="2023-07-02T14:00:00+00:00"><title>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-image Diffusion Models (English) | Hila Chefer</title></head><body id=top data-spy=scroll data-target=#TableOfContents data-offset=71><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main><div class=container><a class=navbar-brand href=/>Hila Chefer</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class="pub py-5" itemscope itemtype=http://schema.org/Event><div class="article-container py-3"><h1 itemprop=name>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-image Diffusion Models (English)</h1><meta content="2023-07-02 14:00:00 +0000 UTC" itemprop=datePublished><meta content="2023-07-02 14:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span></div></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href="https://www.youtube.com/watch?v=RkI4BFG0GP0" target=_blank rel=noopener>Video</a></div></div></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract itemprop=text>Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g. colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention- based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen — or excite — their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.</p><div class=row><div class="col-md-10 mx-auto"><div class=row><div class="col-12 col-md-3 pub-row-heading">Date</div><div class="col-12 col-md-9" itemprop=datePublished>Jul 2, 2023<div class=talk-time>14:00
&mdash; 15:00</div></div></div></div></div><div class="d-md-none space-below"></div><div class=row><div class="col-md-10 mx-auto"><div class=row><div class="col-12 col-md-3 pub-row-heading">Event</div><div class="col-12 col-md-9">Data Science Bond Microsoft</div></div></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Location</div><div class="col-12 col-md-9">Online event</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tags/stable-diffusion/>Stable Diffusion</a>
<a class="badge badge-light" href=/tags/attention/>Attention</a>
<a class="badge badge-light" href=/tags/semantic-guidance/>Semantic Guidance</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><div class=media-body><h5 class=card-title itemprop=name><a href=/authors/hila-chefer/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div></div></div><script src=/js/mathjax-config.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin=anonymous async></script><script>hljs.initHighlightingOnLoad()</script><script src=/js/academia.min.5328943609f83580d0f13f6d5b5f2587.js></script><div class=container><footer class=site-footer><div class=container><div class="row align-items-center"><div class="col-md-6 mb-4 mb-md-0"><p class=mb-0>Copyright © 2024 &#183;
Powered by
<a href=https://gethugothemes.com target=_blank rel=noopener>Gethugothemes</a></p></div><div class=col-md-6><ul class="list-inline network-icon text-right mb-0"><li class=list-inline-item><a href=https://github.com/hila-chefer target=_blank rel=noopener title=Github><i class="fab fa-github" aria-hidden=true></i></a></li><li class=list-inline-item><a href="https://scholar.google.com/citations?user=B8sA9JoAAAAJ&amp;hl=en" target=_blank rel=noopener title="Google Scholar"><i class="ai ai-google-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.semanticscholar.org/author/Hila-Chefer/2038268012 target=_blank rel=noopener title="Semantic Scholar"><i class="ai ai-semantic-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://twitter.com/hila_chefer target=_blank rel=noopener title=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.linkedin.com/in/hila-chefer target=_blank rel=noopener title=LinkedIn><i class="fab fa-linkedin" aria-hidden=true></i></a></li></ul></div></div></div></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>