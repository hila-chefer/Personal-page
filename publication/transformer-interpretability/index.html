<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academia 4.3.1"><meta name=generator content="Hugo 0.101.0"><meta name=author content="Hila Chefer"><meta name=description content="This paper presents an interpretability method for self-attention based models, and specifically for Transformer encoders. The method incorporates LRP and gradients, and achieves SOTA results for ViT, BERT, and DeiT."><link rel=alternate hreflang=en-us href=https://examplesite.org/publication/transformer-interpretability/><meta name=theme-color content="#b54845"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap"><link rel=stylesheet href=/css/academia.min.d1d8d530dfa7b55780d5887faa05b19b.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://examplesite.org/publication/transformer-interpretability/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@hila_chefer"><meta property="twitter:creator" content="@hila_chefer"><meta property="og:site_name" content="Hila Chefer"><meta property="og:url" content="https://examplesite.org/publication/transformer-interpretability/"><meta property="og:title" content="Transformer Interpretability Beyond Attention Visualization | Hila Chefer"><meta property="og:description" content="This paper presents an interpretability method for self-attention based models, and specifically for Transformer encoders. The method incorporates LRP and gradients, and achieves SOTA results for ViT, BERT, and DeiT."><meta property="og:image" content="https://examplesite.org/publication/transformer-interpretability/featured.png"><meta property="twitter:image" content="https://examplesite.org/publication/transformer-interpretability/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2021-06-01T00:00:00+00:00"><meta property="article:modified_time" content="2021-06-01T00:00:00+00:00"><title>Transformer Interpretability Beyond Attention Visualization | Hila Chefer</title></head><body id=top data-spy=scroll data-target=#TableOfContents data-offset=71><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main><div class=container><a class=navbar-brand href=/>Hila Chefer</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class=pub itemscope itemtype=http://schema.org/CreativeWork><div class="container split-header"><div class="row justify-content-center"><div class=col-lg-8><img class="img-fluid w-100" src=/publication/transformer-interpretability/featured_hu802f1524ea8eb9f338b74ff95afb04c0_199607_950x500_fit_q90_lanczos_3.png itemprop=image alt></div><div class=col-lg-8><h1 itemprop=name>Transformer Interpretability Beyond Attention Visualization</h1><meta content="2021-06-01 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2021-06-01 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/shir-gur/>Shir Gur</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/lior-wolf/>Lior Wolf</a></span></div><span class=article-date><time>June 2021</time></span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/publication/transformer-interpretability/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/hila-chefer/Transformer-Explainability target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://analyticsindiamag.com/compute-relevancy-of-transformer-networks-via-novel-interpretable-transformer/ target=_blank rel=noopener>AIM Article</a></div></div></div></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract itemprop=text>Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.</p><div class=row><div class="col-md-10 mx-auto"><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tags/explainability/>Explainability</a>
<a class="badge badge-light" href=/tags/transformers/>Transformers</a>
<a class="badge badge-light" href=/tags/vit/>ViT</a>
<a class="badge badge-light" href=/tags/bert/>BERT</a>
<a class="badge badge-light" href=/tags/self-attention/>Self-Attention</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><div class=media-body><h5 class=card-title itemprop=name><a href=/authors/hila-chefer/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
<script>hljs.initHighlightingOnLoad()</script><script src=/js/academia.min.3a051f57fc38e3443ee465fe9de1f3ba.js></script><div class=container><footer class=site-footer><div class=container><div class="row align-items-center"><div class="col-md-6 mb-4 mb-md-0"><p class=mb-0>Copyright © 2022 &#183;
Powered by
<a href=https://gethugothemes.com target=_blank rel=noopener>Gethugothemes</a></p></div><div class=col-md-6><ul class="list-inline network-icon text-right mb-0"><li class=list-inline-item><a href=https://github.com/hila-chefer target=_blank rel=noopener title=Github><i class="fab fa-github" aria-hidden=true></i></a></li><li class=list-inline-item><a href="https://scholar.google.com/citations?user=B8sA9JoAAAAJ&hl=en" target=_blank rel=noopener title="Google Scholar"><i class="ai ai-google-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.semanticscholar.org/author/Hila-Chefer/2038268012 target=_blank rel=noopener title="Semantic Scholar"><i class="ai ai-semantic-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://twitter.com/hila_chefer target=_blank rel=noopener title=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.linkedin.com/in/hila-chefer target=_blank rel=noopener title=LinkedIn><i class="fab fa-linkedin" aria-hidden=true></i></a></li></ul></div></div></div></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>