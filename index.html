<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hila Chefer</title>

    <meta name="author" content="Hila Chefer">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hila Chefer
                </p>
				<p>Hi there! I am a PhD Candidate at Tel Aviv University, working in the Deep Learning Lab under the supervision of <a href="https://www.cs.tau.ac.il/~wolf/">Prof. Lior Wolf</a>. Also, I am currently a research intern at <a href="https://ai.google/research">Google Research</a> in Tel Aviv.

					My research is centered around computer vision and multi-modal learning. I am particularly passionate about developing tools to enhance interpretability, reliability and controllability of deep foundation models, using the model's internal representations.
					My work has been covered by <a href="https://www.theverge.com/2024/1/27/24052140/google-lumiere-ai-video-generation-runway-pika">The Verge</a>, <a href="https://www.zdnet.com/article/googles-ai-video-generator-tech-is-pretty-amazing-see-for-yourself/">ZDNet</a>, <a href="https://analyticsindiamag.com/ai-mysteries/compute-relevancy-of-transformer-networks-via-novel-interpretable-transformer/">Analytics India Magazine</a>, and others.  
				</p>
               </p>
                <p style="text-align:center">
			          <a href="mailto:hilach70@gmail.com">Email</a> &nbsp/&nbsp
                  		  <a href="https://scholar.google.com/citations?user=B8sA9JoAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
				  <a href="https://twitter.com/hila_chefer">Twitter</a> &nbsp;/&nbsp;
				  <a href="https://github.com/hila-chefer">GitHub</a> &nbsp;/&nbsp;
				  <a href="https://www.semanticscholar.org/author/Hila-Chefer/2038268012">Semantic Scholar</a> &nbsp;/&nbsp;
				  <a href="https://www.linkedin.com/in/hila-chefer/">LinkedIn</a> &nbsp;&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/hilac.jpeg"><img style="width:70%;max-width:100%;object-fit:" alt="profile photo" src="images/hilac.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
		 
		  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h1>Selected Publications</h1>
                <p>
                * indicates equal contribution.  
				</p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


			 <tr>
				<td style="padding:2%;width:45%;vertical-align:middle">
					<img src='images/lumiere.gif' width="100%" height="auto">
				  </td>
				  <td style="padding:2%;width:75%;vertical-align:middle">
					<papertitle>Lumiere: A Space-Time Diffusion Model for Video Generation</papertitle>
					<br>
				  <a href="https://omerbt.github.io/">Omer Bartal*</a>,
				  <strong><span style="font-size: 15px">Hila Chefer*,</span></strong>
				  <a href="https://scholar.google.com/citations?hl=en&user=lbo_R54AAAAJ">Omer Tov*</a>,
				  <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en">Charles Herrmann</a>,
				  <a href="https://scholar.google.com/citations?user=-KSDNZQAAAAJ&hl=en">Roni Paiss</a>,
				  <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>,
				  <a href="http://www.cs.huji.ac.il/~arielephrat/">Ariel Efrat</a>,
				  
				  <a href="https://hurjunhwa.github.io/">Junhwa Hur </a>,
				  <a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>
				  <a href="https://cris.technion.ac.il/en/persons/tomer-michaeli">Tomer Michaeli</a>,
				  <a href="https://www.oliverwang.info/">Oliver Wang</a>,
				  <a href="https://deqings.github.io/">Deqing Sun</a>,
					<a href="https://www.weizmann.ac.il/math/dekel/home">Tali Dekel</a>,
				  <a href="https://inbar-mosseri.github.io/">Inbar Mosseri</a>
				  <br>
				  <em> arxiv, 2024 </em> <br>
				  <a href="https://lumiere-video.github.io/">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2401.12945">Paper</a>
				  <p></p>
				</td>
		    </tr> 
			 <tr>
				<td style="padding:2%;width:35%;vertical-align:middle">
				  <img src='images/teaser_hidden_language.gif' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>The Hidden Language of Diffusion Models</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>,
				  <a href="https://scholar.google.com/citations?user=gypv57sAAAAJ">Oran Lang</a>,
				  <a href="https://mega002.github.io/">Mor Geva</a>,
				  <a href="https://www.linkedin.com/in/volodymyr-polosukhin-b685511b9/">Volodymyr Polosukhin</a>,
				  <br>
				  <a href="https://assafshocher.github.io/">Assaf Shocher</a>,
				  <a href="http://www.weizmann.ac.il/math/irani/">Michal Irani</a>,
				  <a href="https://inbar-mosseri.github.io/">Inbar Mosseri</a>,
				  <a href="http://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
				  <br>
				  <em> ICLR, 2024 </em> <br>
				  <a href="https://hila-chefer.github.io/Conceptor/">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2306.00966">Paper</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <img src='images/attend_and_excite.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer*</span></strong>,
				  <a href="https://yuval-alaluf.github.io/research.html">Yuval Alaluf*</a>,
				  <a href="https://yael-vinker.github.io/website/">Yael Vinker</a>,
				  <a href="https://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>,
				  <a href="https://danielcohenor.com/publications/">Daniel Cohen-Or</a>
				  <br>
				<em> SIGGRAPH (journal), 2023 </em> <br>
				  <a href="https://attendandexcite.github.io/Attend-and-Excite/">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2301.13826">Paper</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <img src='images/robust_vit_crop.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>Optimizing Relevance Maps of Vision Transformers Improves Robustness</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>,
				  <a href="https://idansc.github.io/">Idan Schwartz</a>,
				  <a href="https://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
				  <br>
				  <em> NeurIPS, 2022 </em> <br>
				  <a href="https://github.com/hila-chefer/RobustViT">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2206.01161">Paper</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <img src='images/target_clip.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>Image-Based Clip-Guided Essence Transfer</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>,
				  <a href="https://sagiebenaim.github.io/">Sagie Benaim</a>,
				  <a href="https://scholar.google.com/citations?user=-KSDNZQAAAAJ&hl=en">Roni Paiss</a>,
				  <a href="https://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
				  <br>
				  <em> ECCV, 2022 </em> <br>
				  <a href="https://github.com/hila-chefer/TargetCLIP">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2110.12427">Paper</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <img src='images/no_token.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>No Token Left Behind: Explainability-Aided Image Classification and Generation</papertitle>
				  <br>
				  <a href="https://scholar.google.com/citations?user=-KSDNZQAAAAJ&hl=en">Roni Paiss</a>,
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>,
				  <a href="https://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
				  <br>
				  <em> ECCV, 2022 </em> <br>
				  <a href="https://arxiv.org/abs/2204.04908">Paper</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <img src='images/generic_expl.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>,
				  <a href="https://www.gurshir.com/">Shir Gur</a>,
				  <a href="https://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
				  <br>
				  <em> ICCV, 2021 (Oral) </em> <br>
				  <a href="https://github.com/hila-chefer/Transformer-MM-Explainability">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2103.15679">Paper</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <img src='images/transformer_expl.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:0%;vertical-align:middle">
				  <papertitle>Transformer Interpretability Beyond Attention Visualization</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>,
				  <a href="https://www.gurshir.com/">Shir Gur</a>,
				  <a href="https://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
				  <br>
				  <em> CVPR, 2021 </em> <br>
				  <a href="https://github.com/hila-chefer/Transformer-Explainability">Project page</a>
				  /
				  <a href="https://arxiv.org/abs/2012.09838">Paper</a>
				  <p></p>
				</td>
			  </tr> 
          </tbody></table>

		  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
			<td style="padding:20px;width:100%;vertical-align:middle">
			  <h1>Recent Talks</h1>
			</td>
		  </tr>
		</tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>.           <tr>
			<td style="padding:2%;width:45%;vertical-align:middle">
				<img src='images/cae10efc-b173-448c-96fb-e20091c24105.jpeg' width="100%" height="auto">
			  </td>
			  <td style="padding:2%;width:75%;vertical-align:middle">
				<papertitle>Lumiere: A Space-Time Diffusion Model for Video Generation</papertitle>
				<br>
				<strong><span style="font-size: 15px">Hila Chefer</span></strong>
				<br>
				<a href="https://youtu.be/-zbCjBULJBk">Recording</a>
				<p></p>
			  </td>
		  </tr> 
		   <tr>
			<td style="padding:2%;width:45%;vertical-align:middle">
				<img src='images/atv.jpeg' width="100%" height="auto">
			  </td>
			  <td style="padding:2%;width:75%;vertical-align:middle">
				<papertitle>All Things ViTs: Understanding and Interpreting Attention in Vision</papertitle>
				<br>
				<strong><span style="font-size: 15px">Hila Chefer*,</span></strong>
				<a href="https://sayak.dev/">Sayak Paul*</a>
				<br>
				<em> CVPR Tutorial, 2023 </em> <br>
				<a href="https://all-things-vits.github.io/atv/">Tutorial page</a>
				/
				<a href="https://youtu.be/ma3NYVo8Im0">Recording</a>
				<p></p>
			  </td>
		  </tr> 
		   <tr>
			  <td style="padding:2%;width:30%;vertical-align:middle">
				<img src='images/columbia.jpeg' width="100%" height="auto">
			  </td>
			  <td style="padding:2%;width:75%;vertical-align:middle">
				<papertitle>Transformer Explainability Beyond Accountability</papertitle>
				<br>
				<strong><span style="font-size: 15px">Hila Chefer</span></strong>
				<br>
				<em> Columbia Vision Seminar </em> <br>
				<a href="https://m.youtube.com/watch?v=A1tqsEkSoLg">Recording</a>
				<p></p>
			  </td>
			</tr> 
			<tr>
				<td style="padding:2%;width:30%;vertical-align:middle">
				  <img src='images/microsoft.png' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:75%;vertical-align:middle">
				  <papertitle>Intro to Transformers and Transformer Explainability</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>
				  <br>
				  <em> Microsoft Data Science Bond </em> <br>
				  <a href="https://youtu.be/a0O_QhE9XFM">Recording</a>
				  <p></p>
				</td>
			  </tr> 
			  <tr>
				<td style="padding:2%;width:30%;vertical-align:middle">
				  <img src='images/voxel.jpeg' width="100%" height="auto">
				</td>
				<td style="padding:2%;width:75%;vertical-align:middle">
				  <papertitle>Leveraging Attention for Improved Accuracy and Robustness</papertitle>
				  <br>
				  <strong><span style="font-size: 15px">Hila Chefer</span></strong>
				  <br>
				  <em> Voxel51 Computer Vision Meetup</em> <br>
				  <a href="https://www.youtube.com/watch?v=QYXIHIvesYo">Recording</a>
				  <p></p>
				</td>
			  </tr> 
			  <td style="padding:2%;width:30%;vertical-align:middle">
				<img src='images/microsoft.png' width="100%" height="auto">
			  </td>
			  <td style="padding:2%;width:75%;vertical-align:middle">
				<papertitle>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-image Diffusion Models</papertitle>
				<br>
				<strong><span style="font-size: 15px">Hila Chefer</span></strong>
				<br>
				<em> Microsoft Data Science Bond </em> <br>
				<a href="https://www.youtube.com/watch?v=RkI4BFG0GP0">Recording</a>
				<p></p>
			  </td>
			</tr> 
		</tbody></table>

		<h2 align="center">Contact: hilach70 at gmail dot com</h2>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="1">
                    <a href="https://jonbarron.info/">webpage template from Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
			</tbody>
		  </table>
        </td>
      </tr>
    </table>
  </body>
</html>
